{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from dataset import VAdatasetDecoder\n",
    "from transformer import VertexFitting\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import font_manager\n",
    "import matplotlib\n",
    "from trainer import plot_scatter, plot_scatter_len, set_seed\n",
    "\n",
    "# set parameters for matplotlib\n",
    "plt.rcdefaults()\n",
    "from pathlib import Path\n",
    "font_path = str(Path(matplotlib.get_data_path(), \"fonts/ttf/cmr10.ttf\"))\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = prop.get_name()\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "params = {'mathtext.default': 'regular' }          \n",
    "plt.rcParams.update(params)\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# manually specify the GPUs to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-c\", \"--dataset_path\", required=True, type=str, help=\"train dataset path\")\n",
    "parser.add_argument(\"-im\", \"--img_size\", type=int, default=5, help=\"image size (one dimension)\")\n",
    "parser.add_argument(\"-is\", \"--input_size\", type=int, default=1, help=\"input dimension (per cube)\")\n",
    "parser.add_argument(\"-ts\", \"--target_size\", type=int, default=7, help=\"target size\")\n",
    "parser.add_argument(\"-ns\", \"--noise_size\", type=int, default=100, help=\"size of the noise\")\n",
    "parser.add_argument(\"-hs\", \"--hidden\", type=int, default=256, help=\"hidden size of transformer model\")\n",
    "parser.add_argument(\"-dr\", \"--dropout\", type=float, default=0.1, help=\"dropout of the model\")\n",
    "parser.add_argument(\"-el\", \"--encoder_layers\", type=int, default=8, help=\"number of encoder layers\")\n",
    "parser.add_argument(\"-dl\", \"--decoder_layers\", type=int, default=8, help=\"number of decoder layers\")\n",
    "parser.add_argument(\"-a\", \"--attn_heads\", type=int, default=8, help=\"number of attention heads\")\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64, help=\"number of batch_size\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"number of epochs\")\n",
    "parser.add_argument(\"-w\", \"--num_workers\", type=int, default=5, help=\"dataloader worker size\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-3, help=\"learning rate of adam\")\n",
    "parser.add_argument(\"-wd\", \"--weight_decay\", type=float, default=0.01, help=\"weight_decay of adam\")\n",
    "parser.add_argument(\"-b1\", \"--beta1\", type=float, default=0.9, help=\"adam first beta value\")\n",
    "parser.add_argument(\"-b2\", \"--beta2\", type=float, default=0.999, help=\"adam second beta value\")   \n",
    "parser.add_argument(\"--eps\", type=float, default=1e-9, help=\"value to prevent division by zero\")\n",
    "parser.add_argument(\"-dis\", \"--disable\", type=bool, default=False, help=\"whether to show the training progress\")\n",
    "parser.add_argument(\"-lo\", \"--load\", type=bool, default=False, help=\"whether to load a pretrained model\")\n",
    "parser.add_argument(\"-s\", \"--save\", type=bool, default=False, help=\"whether to save the model after every epoch\")\n",
    "parser.add_argument(\"-sp\", \"--save_path\", type=str, default=\".\", help=\"whether to save the model after every epoch\")\n",
    "parser.add_argument(\"-es\", \"--early_stopping\", type=int, default=10, help=\"early stopping count\")\n",
    "parser.add_argument('-sr','--source_range', nargs='+', type=int, default=[-1,1], help='source range')\n",
    "parser.add_argument('-tr','--target_range', nargs='+', type=int, default=[0,1], help='source range')\n",
    "parser.add_argument('-tc','--total_charge', type=bool, default=False, help='Use total charge in discriminator')\n",
    "parser.add_argument('-cr','--crit_repeats', type=int, default=5, help='Critic repetitions')\n",
    "parser.add_argument('-mp','--max_protons', type=int, default=3, help='Maximum number of protons')\n",
    "parser.add_argument('-ws','--warmup_steps', type=int, default=1000, help='Maximum number of warmup steps')\n",
    "\n",
    "args = parser.parse_args([\"-c\", \"/scratch2/salonso/vertex_activity/images_5M/event{}.npz\",\n",
    "                          \"-lo\", 0,\n",
    "                          \"-b\", \"512\",\n",
    "                          \"-ts\", \"3\",\n",
    "                          \"-wd\", \"0\",\n",
    "                          \"-dis\", 0,\n",
    "                          \"-s\", \"True\",\n",
    "                          \"-sp\", \"pretrained/full_transformer_5M_full_best\",\n",
    "                          \"-e\", \"5000\",\n",
    "                          \"-ns\", \"64\",\n",
    "                          \"-el\", \"5\",\n",
    "                          \"-dl\", \"5\",\n",
    "                          \"-w\", \"16\",\n",
    "                          \"-hs\", \"192\",\n",
    "                          \"-a\", \"12\",\n",
    "                          \"-is\", \"1\",\n",
    "                          \"-lr\", \"0.0001\",\n",
    "                          \"--eps\", \"1e-12\",\n",
    "                          \"-tr\", \"0\", \"1\",\n",
    "                          \"-tc\", 0,\n",
    "                          \"-mp\", \"5\",\n",
    "                          \"-ws\", \"2000\",\n",
    "                          \"-im\", \"7\",\n",
    "                         ]\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b375b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini dataset\n",
    "PAD_IDX = -2\n",
    "dataset = VAdatasetDecoder(args.dataset_path, source_range=args.source_range, \n",
    "                           target_range=args.target_range, max_protons=args.max_protons, PAD_IDX=PAD_IDX)\n",
    "\n",
    "# sets\n",
    "fulllen = len(dataset)\n",
    "train_len = int(fulllen*0.6)\n",
    "val_len = int(fulllen*0.1)\n",
    "test_len = fulllen-train_len-val_len\n",
    "train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len],\n",
    "                                            generator=torch.Generator().manual_seed(7))\n",
    "\n",
    "# loaders\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, collate_fn=dataset.collate_fn2, shuffle=True)\n",
    "valid_loader = DataLoader(val_set, batch_size=args.batch_size, collate_fn=dataset.collate_fn2, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=args.batch_size, collate_fn=dataset.collate_fn2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Auxiliary mask functions needed for the transformer network.\n",
    "'''\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]+1\n",
    "    tgt_seq_len = tgt.shape[0]+1\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "    \n",
    "    src_padding_mask = torch.zeros(size=(src.size(1), src.size(0)+1), dtype=torch.bool).to(device)\n",
    "    src_padding_mask[:, 1:] = (src[:,:,0] == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    tgt_padding_mask = torch.zeros(size=(tgt.size(1), tgt.size(0)+1), dtype=torch.bool).to(device)\n",
    "    tgt_padding_mask[:, 1:] = (tgt[:,:,0] == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    src[src==PAD_IDX] = 0\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def create_mask_src(src):\n",
    "    src_seq_len = src.shape[0]+1\n",
    "    \n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "    src_padding_mask = torch.zeros(size=(src.size(1), src.size(0)+1), dtype=torch.bool).to(device)\n",
    "    src_padding_mask[:, 1:] = (src[:,:,0] == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    src[src==PAD_IDX] = 0\n",
    "    \n",
    "    return src_mask, src_padding_mask\n",
    "\n",
    "def create_mask_tgt(tgt):\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    tgt_padding_mask = (tgt[:,:,0] == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    return tgt_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f835ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = VertexFitting(num_encoder_layers = args.encoder_layers,\n",
    "                           num_decoder_layers = args.decoder_layers,\n",
    "                           emb_size = args.hidden,\n",
    "                           nhead = args.attn_heads,\n",
    "                           img_size = args.img_size,\n",
    "                           src_size = args.input_size,\n",
    "                           tgt_size = args.target_size,\n",
    "                           dropout = args.dropout,\n",
    "                           maxlen = 5,\n",
    "                           device = device, \n",
    "                           )\n",
    "\n",
    "print(model)\n",
    "model = model.to(device)\n",
    "\n",
    "model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "print(\"total trainable params: {} (model).\".format(model_total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading saved model...\")\n",
    "checkpoint = torch.load(args.save_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']+1\n",
    "train_losses = checkpoint['train_losses']\n",
    "val_losses = checkpoint['val_losses']\n",
    "min_val_loss = min(val_losses)\n",
    "count = checkpoint['count']\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training and Validation Loss\", fontsize=20)\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.axvline(np.array(val_losses).argmin(), color=\"red\", ls=\"--\", label=\"best\")\n",
    "plt.xlabel(\"epochs\", fontsize=15)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(3e-1,1)\n",
    "#plt.ylim(0.215,0.23)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22257cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions needed to evaluate the network.\n",
    "''' \n",
    "\n",
    "def eval_event(event_n, model, test_set):\n",
    "    \"\"\"Evaluate the model on a single event.\n",
    "\n",
    "    Args:\n",
    "        event_n (int): the event number (from the test set).\n",
    "        model: transformer neural network.\n",
    "        test_set: the set used for testing.\n",
    "    \"\"\"\n",
    "    # model to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # retrieve testing event from the dataset\n",
    "    event = test_set[event_n]\n",
    "    X, Y, vtx = event['images'], event['params'], event['ini_pos']\n",
    "    event = dataset.collate_fn2([event])\n",
    "    src, vtx, tgt, next_tgt, _ = event\n",
    "    \n",
    "    # set up the input\n",
    "    tgt_input = tgt[:-1, :]\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    src_padding_mask = src_padding_mask.to(device)\n",
    "\n",
    "    '''\n",
    "    Run the model (encoder part) on the input event\n",
    "    \n",
    "    Output:\n",
    "        - memory: encoding learnt from the input that will be passed to the decoder.\n",
    "        - vtx_pred: predicted vertex position.\n",
    "    ''' \n",
    "    memory, vtx_pred = model.encode(src, src_mask, src_padding_mask)\n",
    "    memory = memory.to(device)\n",
    "        \n",
    "    '''\n",
    "    Run the model (decoder part) iteratively on the memory\n",
    "    \n",
    "    Output:\n",
    "        - out: physics parameters of the decoded (subtracted) proton.\n",
    "        - is_next: whether to keep decoding or not.\n",
    "    ''' \n",
    "    ys = model.first_token.reshape(1,1,3)\n",
    "    max_len = len(tgt)\n",
    "    for i in range(max_len):\n",
    "        # set up the input\n",
    "        \n",
    "        tgt_mask, tgt_padding_mask = create_mask_tgt(ys)\n",
    "        \n",
    "        # run the model\n",
    "        out, is_next = model.decode(ys, memory, tgt_mask, tgt_padding_mask, src_padding_mask)\n",
    "              \n",
    "        # store output\n",
    "        out_last = out[-1].reshape(1, out.shape[1], out.shape[2])\n",
    "        is_next_last = is_next[-1,0]\n",
    "        ys = torch.cat([ys, out_last], dim=0)\n",
    "        \n",
    "        # loop exiting condition\n",
    "        if is_next_last.argmax(0) == 0:\n",
    "            break\n",
    "            \n",
    "    # parse output\n",
    "    X = X[:,:,0]\n",
    "    Y = Y\n",
    "    ys = ys.detach().cpu().numpy()[1:,0,:]\n",
    "    vtx = vtx\n",
    "    vtx_pred = vtx_pred.detach().cpu().numpy()\n",
    "    Y[:, 0] = np.interp(Y[:, 0].ravel(), dataset.source_range, \n",
    "                       (dataset.min_KE, dataset.max_KE)).reshape(Y[:, 0].shape)\n",
    "    Y[:, 1] = np.interp(Y[:, 1].ravel(), dataset.source_range, \n",
    "                       (dataset.min_theta, dataset.max_theta)).reshape(Y[:, 1].shape)\n",
    "    Y[:, 2] = np.interp(Y[:, 2].ravel(), dataset.source_range, \n",
    "                       (dataset.min_phi, dataset.max_phi)).reshape(Y[:, 2].shape)\n",
    "    ys[:, 0] = np.interp(ys[:, 0].ravel(), dataset.source_range, \n",
    "                       (dataset.min_KE, dataset.max_KE)).reshape(ys[:, 0].shape)\n",
    "    ys[:, 1] = np.interp(ys[:, 1].ravel(), dataset.source_range, \n",
    "                       (dataset.min_theta, dataset.max_theta)).reshape(ys[:, 1].shape)\n",
    "    ys[:, 2] = np.interp(ys[:, 2].ravel(), dataset.source_range, \n",
    "                       (dataset.min_phi, dataset.max_phi)).reshape(ys[:, 2].shape)\n",
    "    vtx = np.interp(vtx.ravel(), dataset.source_range, \n",
    "                       (dataset.min_pos, dataset.max_pos)).reshape(vtx.shape)\n",
    "    vtx_pred = np.interp(vtx_pred.ravel(), dataset.source_range, \n",
    "                       (dataset.min_pos, dataset.max_pos)).reshape(vtx_pred.shape)\n",
    "        \n",
    "    return X, Y, ys, vtx, vtx_pred\n",
    "\n",
    "def eval_batch(batch, model):\n",
    "    \"\"\"Evaluate the model on a single event.\n",
    "\n",
    "    Args:\n",
    "        batch: batch of events to test.\n",
    "        model: transformer neural network.\n",
    "    \"\"\" \n",
    "    # model to evaluate mode\n",
    "    model.eval()\n",
    "   \n",
    "    # retrieve testing batch\n",
    "    src, vtx, tgt, next_tgt, _ = batch\n",
    "    tgt_input = tgt[:-1, :]\n",
    "    src = src.clone()\n",
    "\n",
    "    # set up the input\n",
    "    src_mask, src_padding_mask = create_mask_src(src)\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    src_padding_mask = src_padding_mask.to(device)\n",
    " \n",
    "    '''\n",
    "    Run the model (encoder part) on the input event\n",
    "    \n",
    "    Output:\n",
    "        - memory: encoding learnt from the input that will be passed to the decoder.\n",
    "        - vtx_pred: predicted vertex position.\n",
    "    ''' \n",
    "    memory, vtx_pred = model.encode(src, src_mask, src_padding_mask)\n",
    "    memory = memory.to(device)\n",
    "    \n",
    "    # set up an structure to store the results from the decoder\n",
    "    max_len = len(tgt)\n",
    "    ys = torch.zeros(max_len+1, tgt_input.shape[1], 3).fill_(PAD_IDX).type(torch.float).to(device)\n",
    "    ys_first = model.first_token.repeat(1, tgt_input.shape[1], 1)\n",
    "    ys[0, :, :] = ys_first\n",
    "    \n",
    "    # keep track of predictions that finished (none before starting)\n",
    "    prev_info = torch.ones(size=(src.shape[1],)).bool().to(device)\n",
    "    \n",
    "    del src, src_mask\n",
    "    \n",
    "    '''\n",
    "    Run the model (decoder part) iteratively on the memory\n",
    "    \n",
    "    Output:\n",
    "        - out: physics parameters of the decoded (subtracted) proton.\n",
    "        - is_next: whether to keep decoding or not.\n",
    "    ''' \n",
    "    for i in range(max_len):\n",
    "        # create masks and run model\n",
    "        tgt_mask, tgt_padding_mask = create_mask_tgt(ys[:i+1])\n",
    "        out, is_next = model.decode(ys[:i+1], memory, tgt_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        # reshape output\n",
    "        out_last = out[-1].reshape(1, out.shape[1], out.shape[2])\n",
    "        is_next_last = is_next[-1].argmax(1).bool()\n",
    "    \n",
    "        # only update results of predictions that haven't finished\n",
    "        ys[i+1, prev_info, :] = out_last.detach()[:,prev_info,:]\n",
    "        \n",
    "        # update the information with predictions that just finished\n",
    "        prev_info = torch.logical_and(prev_info, is_next_last) \n",
    "    \n",
    "    return ys, vtx_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example: run the network on one event.\n",
    "\n",
    "Output:\n",
    "  - X: images of the protons involved in the VA. Shape: (#protons, 9, 9).\n",
    "  - Y: physics parameters (KE, theta, phi) of the protons involved in the VA. Shape: (#protons, 3).\n",
    "  - Y_hat: predicted physics parameters (KE, theta, phi) of the protons involved in the VA. Same shape as Y.\n",
    "  - vtx: vertex of the VA. Shape: (1, 3).\n",
    "  - vtx_hat: predicted vertex of the VA. Same shape as vtx_hat.\n",
    "'''\n",
    "\n",
    "event_n = 0\n",
    "np.set_printoptions(suppress = True)\n",
    "set_seed(7, random, np, torch) # for reproducibility\n",
    "X, Y, Y_hat, vtx, vtx_hat = eval_event(event_n, model, test_set)\n",
    "\n",
    "print(\"Vertex true: {}\".format(vtx[0]))\n",
    "print(\"Vertex pred: {}\\n\".format(vtx_hat[0]))\n",
    "    \n",
    "for i in range(max(len(Y), len(Y_hat))):\n",
    "    if i<len(Y) and i<len(Y_hat):\n",
    "        print(\"Proton {0}:\\n (true) KE={1:.2f}, theta={2:.2f}, phi={3:.2f};\"\n",
    "              \"(transformer) KE={4:.2f}, theta={5:.2f}, phi={6:.2f}\".format(i+1, Y[i,0], Y[i,1], Y[i,2],\n",
    "                                                                            Y_hat[i,0], Y_hat[i,1], Y_hat[i,2]))\n",
    "    elif i>=len(Y) and i<len(Y_hat):\n",
    "        print(\"Proton {0}:\\n (transformer) KE={1:.2f},\"\n",
    "              \"theta={2:.2f}, phi={3:.2f}\".format(i+1, Y_hat[i,0], Y_hat[i,1], Y_hat[i,2]))\n",
    "    \n",
    "    elif i<len(Y) and i>=len(Y_hat):\n",
    "        print(\"Proton {0}:\\n (true) KE={1:.2f}, theta={2:.2f}, phi={3:.2f}\".format(i+1, Y[i,0], Y[i,1], Y[i,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test network on the entire test set.\n",
    "'''\n",
    "\n",
    "def test(loader, disable=False):\n",
    "    \n",
    "    batch_size = loader.batch_size\n",
    "    n_batches = int(math.ceil(len(loader.dataset)/batch_size)) #if max_iters_train is None else max_iters_train\n",
    "    t = tqdm.tqdm(enumerate(loader),total=n_batches, disable=disable)\n",
    "    \n",
    "    sum_loss = 0.\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    Ys_hat = []\n",
    "    Vtx = []\n",
    "    Vtx_hat = []\n",
    "    Lens = []\n",
    "    \n",
    "    for i, data in t:        \n",
    "        src, vtx, tgt, next_tgt, lens = data\n",
    "        \n",
    "        y_hat, vtx_hat = eval_batch(data, model)\n",
    "        \n",
    "        src = src.cpu().numpy()\n",
    "        tgt = tgt.cpu().numpy()\n",
    "        y_hat = y_hat.detach().cpu().numpy()\n",
    "        vtx = vtx.cpu().numpy()\n",
    "        vtx_hat = vtx_hat.detach().cpu().numpy()\n",
    "        lens = lens.cpu().numpy()\n",
    "        \n",
    "        for event_n in range(src.shape[1]):\n",
    "            X = src[:,event_n,:]\n",
    "            Y = tgt[:,event_n,:]\n",
    "            Y_hat = y_hat[1:,event_n,:]\n",
    "            Len = lens[:, event_n]\n",
    "            \n",
    "            # remove padding\n",
    "            X = X[X!=PAD_IDX].reshape(-1,2)\n",
    "            Y = Y[Y!=PAD_IDX].reshape(-1,3)\n",
    "            Y_hat = Y_hat[Y_hat!=PAD_IDX].reshape(-1,3)\n",
    "            Len = Len[Len!=PAD_IDX]\n",
    "            \n",
    "            Xs.append(X)\n",
    "            Ys.append(Y)\n",
    "            Ys_hat.append(Y_hat)\n",
    "            Vtx.append(vtx[event_n])\n",
    "            Vtx_hat.append(vtx_hat[event_n])\n",
    "            Lens.append(Len)\n",
    "    \n",
    "    return Xs, Ys, Ys_hat, Vtx, Vtx_hat, Lens\n",
    "\n",
    "set_seed(7, random, np, torch) # for reproducibility\n",
    "Xs, Ys, Ys_hat, Vtx, Vtx_hat, Lens = test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a Pandas dataframe witht he results.\n",
    "'''\n",
    "\n",
    "dic = {'KE_true':[], 'KE_reco':[], 'theta_true':[], 'theta_reco':[], \n",
    "       'phi_true':[], 'phi_reco':[], 'vertex_true':[], 'vertex_reco':[],\n",
    "       'lens':[], 'nparticles_true':[], 'nparticles_reco':[], 'eventid':[]}\n",
    "\n",
    "t = tqdm.tqdm(range(len(Xs)), total=len(Xs), disable=False)\n",
    "    \n",
    "idx_res = 0\n",
    "for i in t:        \n",
    "    y = Ys[i]\n",
    "    y_hat = Ys_hat[i]\n",
    "    vtx = Vtx[i]\n",
    "    vtx_hat = Vtx_hat[i]\n",
    "    lens = Lens[i]\n",
    "    \n",
    "    nparticles_true = y.shape[0]\n",
    "    nparticles_reco = y_hat.shape[0]\n",
    "    min_nparticles = min(nparticles_true, nparticles_reco)\n",
    "    \n",
    "    y = y[:min_nparticles]\n",
    "    y_hat = y_hat[:min_nparticles]\n",
    "    lens = lens[:min_nparticles]\n",
    "        \n",
    "    KE_true = np.interp(y[:, 0].ravel(), dataset.source_range, \n",
    "                       (dataset.min_KE, dataset.max_KE)).reshape(y[:, 0].shape)\n",
    "    KE_reco = np.interp(y_hat[:, 0].ravel(), dataset.source_range, \n",
    "                       (dataset.min_KE, dataset.max_KE)).reshape(y_hat[:, 0].shape)\n",
    "    theta_true = np.interp(y[:, 1].ravel(), dataset.source_range, \n",
    "                       (dataset.min_theta, dataset.max_theta)).reshape(y[:, 1].shape)\n",
    "    theta_reco = np.interp(y_hat[:, 1].ravel(), dataset.source_range, \n",
    "                       (dataset.min_theta, dataset.max_theta)).reshape(y_hat[:, 1].shape)\n",
    "    phi_true = np.interp(y[:, 2].ravel(), dataset.source_range, \n",
    "                       (dataset.min_phi, dataset.max_phi)).reshape(y[:, 2].shape)\n",
    "    phi_reco = np.interp(y_hat[:, 2].ravel(), dataset.source_range, \n",
    "                       (dataset.min_phi, dataset.max_phi)).reshape(y_hat[:, 2].shape)\n",
    "    vertex_true = np.interp(vtx.ravel(), dataset.source_range, \n",
    "                       (dataset.min_pos, dataset.max_pos)).reshape(1, 3)\n",
    "    vertex_reco = np.interp(vtx_hat.ravel(), dataset.source_range, \n",
    "                       (dataset.min_pos, dataset.max_pos)).reshape(1, 3)\n",
    "        \n",
    "    dic['KE_true'].append(KE_true)\n",
    "    dic['KE_reco'].append(KE_reco)\n",
    "    dic['theta_true'].append(theta_true)\n",
    "    dic['theta_reco'].append(theta_reco)\n",
    "    dic['phi_true'].append(phi_true)\n",
    "    dic['phi_reco'].append(phi_reco)\n",
    "    dic['vertex_true'].append(vertex_true)\n",
    "    dic['vertex_reco'].append(vertex_reco)\n",
    "    dic['nparticles_true'].append(nparticles_true)\n",
    "    dic['nparticles_reco'].append(nparticles_reco)\n",
    "    dic['lens'].append(lens)\n",
    "    dic['eventid'].append(i)\n",
    "\n",
    "    idx_res+=1\n",
    "    \n",
    "df = pd.DataFrame(dic, columns = ['eventid', 'KE_true', 'KE_reco', 'theta_true', 'theta_reco', \n",
    "                                  'phi_true', 'phi_reco', 'vertex_true', 'vertex_reco',\n",
    "                                  'lens', 'nparticles_true', 'nparticles_reco'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot results (scatterplots)\n",
    "'''\n",
    "\n",
    "KE_true = np.concatenate(df.KE_true.values)\n",
    "KE_reco = np.concatenate(df.KE_reco.values)\n",
    "theta_true = np.concatenate(df.theta_true.values)\n",
    "theta_reco = np.concatenate(df.theta_reco.values)\n",
    "phi_true = np.concatenate(df.phi_true.values)\n",
    "phi_reco = np.concatenate(df.phi_reco.values)\n",
    "vertex_true = np.concatenate(df.vertex_true.values)\n",
    "vertex_reco = np.concatenate(df.vertex_reco.values)\n",
    "lens = np.concatenate(df.lens.values)\n",
    "nparticles_true = df.nparticles_true.values\n",
    "nparticles_reco = df.nparticles_reco.values\n",
    "\n",
    "s = 0.005\n",
    "plot_scatter(KE_true, KE_reco, label=(\"KE\",\"[MeV]\"), s=s)\n",
    "plot_scatter(theta_true, theta_reco, label=(r\"$\\theta$\",\"[rad]\"), s=s)\n",
    "plot_scatter(phi_true, phi_reco, label=(r\"$\\phi$\",\"[rad]\"), s=s)\n",
    "plot_scatter(vertex_true[:,0], vertex_reco[:,0], label=(\"vertex X\",\"[mm]\"), s=s*3)\n",
    "plot_scatter(vertex_true[:,1], vertex_reco[:,1], label=(\"vertex Y\",\"[mm]\"), s=s*3)\n",
    "plot_scatter(vertex_true[:,2], vertex_reco[:,2], label=(\"vertex Z\",\"[mm]\"), s=s*3)\n",
    "plot_scatter_len(lens, KE_reco-KE_true, label=(\"KE\",\"[MeV]\"), s=s*3)\n",
    "plot_scatter_len(lens, theta_reco-theta_true, label=(r\"$\\theta$\",\"[rad]\"), s=s*3)\n",
    "plot_scatter_len(lens, phi_reco-phi_true, label=(r\"$\\phi$\",\"[rad]\"), s=s*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot results (number of protons predicted)\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(nparticles_true, nparticles_reco, digits=3, \n",
    "                            labels=[1,2,3,4,5], target_names=[str(i+1)+\" protons\"for i in range(5)]))\n",
    "conf = confusion_matrix(nparticles_reco, nparticles_true, labels=[1,2,3,4,5])\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92388d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
