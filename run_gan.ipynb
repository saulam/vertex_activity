{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os, sys\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from model import BERT\n",
    "from dataset import VAdatasetFinal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from trainer import plot_event\n",
    "\n",
    "# manually specify the GPUs to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-c\", \"--dataset_path\", required=True, type=str, help=\"train dataset path\")\n",
    "parser.add_argument(\"-im\", \"--img_size\", type=int, default=5, help=\"image size (one dimension)\")\n",
    "parser.add_argument(\"-is\", \"--input_size\", type=int, default=1, help=\"input dimension (per cube)\")\n",
    "parser.add_argument(\"-ls\", \"--label_size\", type=int, default=7, help=\"number of labels\")\n",
    "parser.add_argument(\"-ns\", \"--noise_size\", type=int, default=100, help=\"size of the noise\")\n",
    "parser.add_argument(\"-hs\", \"--hidden\", type=int, default=256, help=\"hidden size of transformer model\")\n",
    "parser.add_argument(\"-dr\", \"--dropout\", type=float, default=0.1, help=\"dropout of the model\")\n",
    "parser.add_argument(\"-l\", \"--layers\", type=int, default=8, help=\"number of layers\")\n",
    "parser.add_argument(\"-a\", \"--attn_heads\", type=int, default=8, help=\"number of attention heads\")\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64, help=\"number of batch_size\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=10, help=\"number of epochs\")\n",
    "parser.add_argument(\"-w\", \"--num_workers\", type=int, default=5, help=\"dataloader worker size\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-3, help=\"learning rate of adam\")\n",
    "parser.add_argument(\"-wd\", \"--weight_decay\", type=float, default=0.01, help=\"weight_decay of adam\")\n",
    "parser.add_argument(\"-b1\", \"--beta1\", type=float, default=0.9, help=\"adam first beta value\")\n",
    "parser.add_argument(\"-b2\", \"--beta2\", type=float, default=0.999, help=\"adam second beta value\")   \n",
    "parser.add_argument(\"--eps\", type=float, default=1e-9, help=\"value to prevent division by zero\")\n",
    "parser.add_argument(\"-dis\", \"--disable\", type=bool, default=False, help=\"whether to show the training progress\")\n",
    "parser.add_argument(\"-lo\", \"--load\", type=bool, default=False, help=\"whether to load a pretrained model\")\n",
    "parser.add_argument(\"-s\", \"--save\", type=bool, default=False, help=\"whether to save the model after every epoch\")\n",
    "parser.add_argument(\"-sp\", \"--save_path\", type=str, default=\".\", help=\"whether to save the model after every epoch\")\n",
    "parser.add_argument(\"-es\", \"--early_stopping\", type=int, default=10, help=\"early stopping count\")\n",
    "parser.add_argument('-sr','--source_range', nargs='+', type=int, default=[-1,1], help='source range')\n",
    "parser.add_argument('-tr','--target_range', nargs='+', type=int, default=[0,1], help='source range')\n",
    "parser.add_argument('-tc','--total_charge', type=bool, default=False, help='Use total charge in discriminator')\n",
    "parser.add_argument('-cr','--crit_repeats', type=int, default=5, help='Critic repetitions')\n",
    "\n",
    "args = parser.parse_args([\"-c\", \"/scratch2/salonso/vertex_activity/images\",\n",
    "                          \"-lo\", 0,\n",
    "                          \"-b\", \"32\",\n",
    "                          \"-ls\", \"6\",\n",
    "                          \"-wd\", \"0\",\n",
    "                          \"-dis\", \"True\",\n",
    "                          \"-s\", \"True\",\n",
    "                          \"-sp\", \"pretrained/gan_tran_rms_final_{}_{}\",\n",
    "                          \"-e\", \"100\",\n",
    "                          \"-ns\", \"512\",\n",
    "                          \"-l\", \"2\",\n",
    "                          \"-w\", \"8\",\n",
    "                          \"-hs\", \"64\",\n",
    "                          \"-is\", \"1\",\n",
    "                          \"-lr\", \"0.00005\",\n",
    "                          \"--eps\", \"1e-8\",\n",
    "                          \"-tr\", \"-1\", \"1\",\n",
    "                          \"-tc\", 0,\n",
    "                          \"-cr\", \"10\",\n",
    "                         ]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datasets\n",
    "\n",
    "dataset_original = VAdatasetFinal(\"/scratch2/salonso/vertex_activity/images\", source_range=args.source_range, \n",
    "                    target_range=args.target_range)\n",
    "dataset_10Mev = VAdatasetFinal(\"/scratch2/salonso/vertex_activity/images_test/10MeV\", source_range=args.source_range, \n",
    "                    target_range=args.target_range)\n",
    "dataset_20Mev = VAdatasetFinal(\"/scratch2/salonso/vertex_activity/images_test/20MeV\", source_range=args.source_range, \n",
    "                    target_range=args.target_range)\n",
    "dataset_30Mev = VAdatasetFinal(\"/scratch2/salonso/vertex_activity/images_test/30MeV\", source_range=args.source_range, \n",
    "                    target_range=args.target_range)\n",
    "dataset_40Mev = VAdatasetFinal(\"/scratch2/salonso/vertex_activity/images_test/40MeV\", source_range=args.source_range, \n",
    "                    target_range=args.target_range)\n",
    "dataset_50Mev = VAdatasetFinal(\"/scratch2/salonso/vertex_activity/images_test/50MeV\", source_range=args.source_range, \n",
    "                    target_range=args.target_range)\n",
    "\n",
    "datasets = [dataset_10Mev, dataset_20Mev, dataset_30Mev, dataset_40Mev, dataset_50Mev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based Generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.decoder = Decoder(self.bert.hidden, 1, activation=True)\n",
    "\n",
    "    def forward(self, label, noise):\n",
    "        x = self.bert(input=None, label=label, noise=noise)        \n",
    "        return self.decoder(x).view(x.shape[0], -1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden, outsize, activation=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, outsize)\n",
    "        if activation is not None:\n",
    "            if args.target_range==[0,1]:\n",
    "                self.activation = nn.Sigmoid()\n",
    "            elif args.target_range==[-1,1]:\n",
    "                self.activation = nn.Tanh()\n",
    "            else:\n",
    "                assert False\n",
    "        else:\n",
    "            self.activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini model\n",
    "\n",
    "print(\"Building BERT model\")\n",
    "bert_gen = BERT(input_size=args.input_size, label_size=args.label_size, noise_size=args.noise_size, hidden=args.hidden,\n",
    "            n_layers=args.layers, attn_heads=args.attn_heads, total_charge=args.total_charge, dropout=args.dropout, device=device)\n",
    "model = Generator(bert_gen).to(device)\n",
    "\n",
    "model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "print(\"total trainable params: {} (model).\".format(model_total_params))\n",
    "\n",
    "# load saved model\n",
    "epoch, iteration = 40, 12000\n",
    "checkpoint = torch.load(args.save_path.format(epoch, iteration))\n",
    "model.load_state_dict(checkpoint['g_state_dict'], strict=False)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f43c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the network on a single image!\n",
    "'''\n",
    "\n",
    "# physics parameters (arbitrary)\n",
    "ini_x = 3.9\n",
    "ini_y = 2.1\n",
    "ini_z = -4.0\n",
    "ke = 12.1\n",
    "theta = 2.14\n",
    "phi = -1.03\n",
    "params = np.array([ini_x, ini_y, ini_z, ke, theta, phi])\n",
    "\n",
    "# normalise (range [0,1])\n",
    "params[:3] = np.interp(params[:3].ravel(), (dataset_original.min_pos, dataset_original.max_pos), \n",
    "                       dataset_original.source_range).reshape(params[:3].shape)\n",
    "params[3] = np.interp(params[3], (dataset_original.min_KE, dataset_original.max_KE), \n",
    "                      dataset_original.source_range).reshape(1)\n",
    "params[4] = np.interp(theta, (dataset_original.min_theta, dataset_original.max_theta), \n",
    "                      dataset_original.source_range).reshape(1)\n",
    "params[5] = np.interp(phi, (dataset_original.min_phi, dataset_original.max_phi), \n",
    "                      dataset_original.source_range).reshape(1)\n",
    "\n",
    "# tensors needed to run the network\n",
    "params = torch.tensor([params]).float().to(device)\n",
    "noise = torch.normal(0, 1, size=(len(params), 1, args.noise_size)).to(device) # normal noise!\n",
    "\n",
    "# run the network!\n",
    "sample_image = model(params, noise).data.cpu()\n",
    "\n",
    "# sample image to standard range\n",
    "sample_image = np.interp(sample_image.ravel(), dataset_original.target_range, \n",
    "                         (dataset_original.min_charge_new, \n",
    "                          dataset_original.max_charge_new)).reshape(sample_image.shape)\n",
    "\n",
    "# plot the event!\n",
    "plot_event(sample_image, elev=20, azim=30, img_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8acbee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
