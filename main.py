import argparseimport torchimport osimport mathfrom tqdm import tqdmimport numpy as npfrom model import BERTfrom model import VAModelfrom dataset import VAdatasetfrom trainer import Chi2Loss, PoissonLikelihood_loss, Combined2Losses, Combined3Lossesfrom trainer import ScheduledOptimfrom torch.utils.data import random_splitfrom torch.utils.data import DataLoaderfrom torch import nnfrom trainer import plot_eventfrom trainer import generator_train_step, generator_test_step# manually specify the GPUs to useos.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"os.environ["CUDA_VISIBLE_DEVICES"]="0"device = 'cuda' if torch.cuda.is_available() else 'cpu'def train():    # parameters    parser = argparse.ArgumentParser()    parser.add_argument("-c", "--dataset_path", required=True, type=str, help="train dataset path")    parser.add_argument("-im", "--img_size", type=int, default=5, help="image size (one dimension)")    parser.add_argument("-ls", "--label_size", type=int, default=7, help="number of labels")    parser.add_argument("-ns", "--noise_size", type=int, default=100, help="size of the noise")    parser.add_argument("-hs", "--hidden", type=int, default=256, help="hidden size of transformer model")    parser.add_argument("-dr", "--dropout", type=float, default=0.1, help="dropout of the model")    parser.add_argument("-l", "--layers", type=int, default=8, help="number of layers")    parser.add_argument("-a", "--attn_heads", type=int, default=8, help="number of attention heads")    parser.add_argument("-b", "--batch_size", type=int, default=64, help="number of batch_size")    parser.add_argument("-e", "--epochs", type=int, default=10, help="number of epochs")    parser.add_argument("-w", "--num_workers", type=int, default=5, help="dataloader worker size")    parser.add_argument("-lr", "--learning_rate", type=float, default=1e-3, help="learning rate of adam")    parser.add_argument("-wd", "--weight_decay", type=float, default=0.01, help="weight_decay of adam")    parser.add_argument("-b1", "--beta1", type=float, default=0.9, help="adam first beta value")    parser.add_argument("-b2", "--beta2", type=float, default=0.999, help="adam second beta value")    parser.add_argument("--eps", type=float, default=1e-9, help="value to prevent division by zero")    parser.add_argument("-dis", "--disable", type=bool, default=False, help="whether to show the training progress")    parser.add_argument("-lo", "--load", type=bool, default=False, help="whether to load a pretrained model")    parser.add_argument("-s", "--save", type=bool, default=False, help="whether to save the model after every epoch")    parser.add_argument("-sp", "--save_path", type=str, default=".", help="whether to save the model after every epoch")    parser.add_argument("-es", "--early_stopping", type=int, default=10, help="early stopping count")    args = parser.parse_args(["-c", "../images",                              "-wd", "0",                              "-dis", 0,                              "-s", "True",                              "-sp", "/scratch2/salonso/vertex_activity/models/model_checkpoint",                              "-e", "100",                              "-l", "5",                              "-w", "8",                              "-hs", "64",                              ]                             )    # ini dataset    dataset = VAdataset(args.dataset_path)    # sets    fulllen = len(dataset)    train_len = int(fulllen * 0.6)    val_len = int(fulllen * 0.1)    test_len = fulllen - train_len - val_len    train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len],                                                generator=torch.Generator().manual_seed(7))    # loaders    train_loader = DataLoader(train_set, collate_fn=dataset.collate_fn, batch_size=args.batch_size,                              num_workers=args.num_workers, shuffle=True)    valid_loader = DataLoader(val_set, collate_fn=dataset.collate_fn, batch_size=args.batch_size,                              num_workers=args.num_workers, shuffle=False)    test_loader = DataLoader(test_set, collate_fn=dataset.collate_fn, batch_size=args.batch_size,                             num_workers=args.num_workers, shuffle=False)    # ini model    print("Building BERT model")    bert = BERT(label_size=args.label_size, noise_size=args.noise_size, hidden=args.hidden,                n_layers=args.layers, attn_heads=args.attn_heads, dropout=args.dropout, device=device)    model = VAModel(bert).to(device)    model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)    print(model)    print("total trainable params: {} (model).".format(model_total_params))    # losses    loss_fn1 = torch.nn.CrossEntropyLoss()    loss_fn2 = Chi2Loss()  # torch.nn.MSELoss()    loss_fn = Combined3Losses(loss_fn1, loss_fn2, alpha3=0.1, device=device)    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, betas=(args.beta1, args.beta2),                                 eps=args.eps, weight_decay=args.weight_decay)    g_losses_train = []    g_losses_val = []    min_charge_plot = dataset.min_charge    min_val_loss = np.inf    epoch = 0    for epoch in range(epoch, args.epochs):        print('Starting epoch {}...'.format(epoch))        train_loss, val_loss = 0., 0.        # TRAIN        batch_size = train_loader.batch_size        n_batches = int(math.ceil(len(train_loader.dataset) / batch_size))        # n_batches = 1000        t = tqdm(enumerate(train_loader), total=n_batches, disable=args.disable)        for ite, (images, labels, signal) in t:            # if ite>=n_batches:            #    break            # Train generator            g_loss = generator_train_step(len(images), model, optimizer, images, signal,                                          labels, args.noise_size, loss_fn, device)            t.set_description("g_loss = {0:.5f}".format(g_loss.item()))            train_loss += g_loss.item()        train_loss /= n_batches        batch_size = valid_loader.batch_size        n_batches = int(math.ceil(len(valid_loader.dataset) / batch_size))        # n_batches = 1000        t = tqdm(enumerate(valid_loader), total=n_batches, disable=args.disable)        for ite, (images, labels, signal) in t:            # if ite>=n_batches:            #    break            # Train generator            g_loss = generator_train_step(len(images), model, optimizer, images, signal,                                          labels, args.noise_size, loss_fn, device)            t.set_description("g_loss = {0:.5f}".format(g_loss.item()))            val_loss += g_loss.item()        val_loss /= n_batches        print("EPOCH {0}, Train loss: {1:.5f}, Val loss: {2:.5f}".format(epoch, train_loss, val_loss))        g_losses_train.append(train_loss)        g_losses_val.append(val_loss)        if args.save:            if val_loss < min_val_loss:                min_val_loss = val_loss                print("Saving best model with val_loss: {}".format(val_loss))                torch.save({                    'epoch': epoch,                    'g_state_dict': model.state_dict(),                    'g_optimizer_state_dict': optimizer.state_dict(),                    'g_losses_train': np.array(g_losses_train),                    'g_losses_val': np.array(g_losses_val),                }, args.save_path + "_best")                count = 0            else:                torch.save({                    'epoch': epoch,                    'g_state_dict': model.state_dict(),                    'g_optimizer_state_dict': optimizer.state_dict(),                    'g_losses_train': np.array(g_losses_train),                    'g_losses_val': np.array(g_losses_val),                }, args.save_path + "_last")                count += 1                if count >= args.early_stopping:                    print("Early stopping...")                    break# Press the green button in the gutter to run the script.if __name__ == '__main__':    train()